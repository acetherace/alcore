{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch.nn import functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from pathlib import Path\n",
    "import pdb\n",
    "import cv2\n",
    "\n",
    "import imageio\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEVizHelper:\n",
    "    def __init__(self, recon_base_imgs, n=20, recon_tracking=True, datagen_tracking=True):\n",
    "        \n",
    "        self.recon_tracking = recon_tracking\n",
    "        self.datagen_tracking = datagen_tracking\n",
    "        \n",
    "        # base image (top row, stitched together)\n",
    "        self.recon_base_img = np.concatenate([np.squeeze(x) for x in recon_base_imgs], axis=1)\n",
    "        # base images (separate, not stitched together)\n",
    "        self.recon_base_imgs = recon_base_imgs\n",
    "        \n",
    "        # placeholders for the reconstruction and data generation\n",
    "        # images that will be used to create gifs\n",
    "        self.recon_tracking_imgs = []\n",
    "        self.datagen_tracking_imgs = []\n",
    "        \n",
    "        # linearly spaced coordinates on the unit square were transformed through \n",
    "        # the inverse CDF of the Gaussian to produce values of the latent variables z\n",
    "        self.grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "        self.grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "        # how many points to sample along the latent space\n",
    "        self.n = n\n",
    "        \n",
    "    def execute(self, model, device):\n",
    "        if self.recon_tracking:\n",
    "            # handle reconstruction figures\n",
    "            recon_figure = self.create_recon_figure(model, device)\n",
    "            self.recon_tracking_imgs.append(recon_figure)\n",
    "        if self.datagen_tracking:\n",
    "            # handle data generation figures\n",
    "            datagen_figure = self.create_datagen_figure(model, device)\n",
    "            self.datagen_tracking_imgs.append(datagen_figure)\n",
    "        \n",
    "    def create_recon_figure(self, model, device):\n",
    "        \"\"\"\n",
    "        Returns an image with the real MNIST images on the top row and\n",
    "        the corresponding reconstructed images on the bottom row. These\n",
    "        images can be used to visualize how well the VAE is able to\n",
    "        reconstruct its input data at various points during training.\n",
    "        \"\"\"\n",
    "        recon_imgs = []\n",
    "        for img in self.recon_base_imgs:\n",
    "            img = img.to(device)\n",
    "            recon = model(img.view(-1,784))[0].view(28,28).cpu().detach().numpy()\n",
    "            recon_imgs.append(recon)\n",
    "        tmp = np.concatenate(recon_imgs, axis=1)\n",
    "        figure = np.concatenate([self.recon_base_img, tmp], axis=0)\n",
    "        figure = (figure * 255.).astype('uint8')\n",
    "        return figure\n",
    "    \n",
    "    def create_datagen_figure(self, model, device):\n",
    "        \"\"\"\n",
    "        Returns an image of digits generated across the manifold of \n",
    "        the latent space.\n",
    "        \"\"\"\n",
    "        figure = np.zeros((28*self.n, 28*self.n))\n",
    "        for i, yi in enumerate(self.grid_x):\n",
    "            for j, xi in enumerate(self.grid_y):\n",
    "                z_sample = torch.tensor([[xi, yi]]).to(device)\n",
    "                x_decoded = model.decode(z_sample)\n",
    "                digit = x_decoded.view(28,28).detach().cpu().numpy()\n",
    "                figure[i * 28: (i + 1) * 28,\n",
    "                       j * 28: (j + 1) * 28] = digit\n",
    "        figure = (figure * 255.).astype('uint8')\n",
    "        return figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "epochs = 10\n",
    "seed = 199\n",
    "log_interval=10\n",
    "device='cuda'\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    lambda x: x.round()\n",
    "])\n",
    "\n",
    "ds = datasets.MNIST('../data', train=False, transform=xforms, download=True)\n",
    "recon_base_imgs = []\n",
    "for i in [1, 4, 12, 15, 22]:\n",
    "    img = ds[i][0]\n",
    "    recon_base_imgs.append(img)\n",
    "recon_base_img = np.concatenate([np.squeeze(x.numpy()) for x in recon_base_imgs], axis=1)\n",
    "del ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.imshow(recon_base_img, cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=xforms),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=xforms),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader.dataset), len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model and Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAEEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard encoder module for variational autoencoders with tabular input and\n",
    "    factorized Gaussian posterior.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_size, hidden_sizes, latent_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_size (int): Dimensionality of the input data.\n",
    "            hidden_sizes (list[int]): Sizes of hidden layers (not including the\n",
    "                input layer or the latent layer).\n",
    "            latent_size (int): Size of the latent space.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data_size=data_size\n",
    "        \n",
    "        # construct the encoder\n",
    "        encoder_szs = [data_size] + hidden_sizes\n",
    "        encoder_layers = []\n",
    "        for in_sz,out_sz, in zip(encoder_szs[:-1], encoder_szs[1:]):\n",
    "            encoder_layers.append(nn.Linear(in_sz, out_sz))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        self.encoder_mu = nn.Linear(encoder_szs[-1], latent_size)\n",
    "        self.encoder_logvar = nn.Linear(encoder_szs[-1], latent_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "        \n",
    "    def gaussian_param_projection(self, x):\n",
    "        return self.encoder_mu(x), self.encoder_logvar(x)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        mu, logvar = self.gaussian_param_projection(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "    \n",
    "    \n",
    "class BernoulliVAEDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE decoder module that models a factorized multivariate Bernoulli\n",
    "    distribution with a feed-forward neural net.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_size, hidden_sizes, latent_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_size (int): Dimensionality of the input data.\n",
    "            hidden_sizes (list[int]): Sizes of hidden layers (not including the\n",
    "                input layer or the latent layer).\n",
    "            latent_size (int): Size of the latent space.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # construct the decoder\n",
    "        hidden_sizes = [latent_size] + hidden_sizes\n",
    "        decoder_layers = []\n",
    "        for in_sz,out_sz, in zip(hidden_sizes[:-1], hidden_sizes[1:]):\n",
    "            decoder_layers.append(nn.Linear(in_sz, out_sz))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "        decoder_layers.append(nn.Linear(hidden_sizes[-1], data_size))\n",
    "        decoder_layers.append(nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "\n",
    "class BernoulliVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    VAE module that combines a `VAEEncoder` and a `BernoulliVAEDecoder` resulting\n",
    "    in full VAE.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_size, encoder_szs, latent_size, decoder_szs=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        # if decoder_szs not specified, assume symmetry\n",
    "        if decoder_szs is None:\n",
    "            decoder_szs = encoder_szs[::-1]\n",
    "        \n",
    "        # construct the encoder\n",
    "        self.encoder = VAEEncoder(data_size=data_size, hidden_sizes=encoder_szs,\n",
    "                                  latent_size=latent_size)\n",
    "        \n",
    "        # construct the decoder\n",
    "        self.decoder = BernoulliVAEDecoder(data_size=data_size, latent_size=latent_size,\n",
    "                                           hidden_sizes=decoder_szs)\n",
    "        \n",
    "        self.data_size = data_size\n",
    "        \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encoder(x)\n",
    "        p_x = self.decoder(z)\n",
    "        return p_x, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(p_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(p_x, x, reduction='sum')\n",
    "\n",
    "    # see Appendix B from VAE paper:\n",
    "    # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, mb, figure_interval, viz_helper, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    pb = progress_bar(train_loader, parent=mb)\n",
    "    for batch_idx, (data, _) in enumerate(pb):\n",
    "        data = data.to(device)\n",
    "        data = data.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % figure_interval == 0:\n",
    "            viz_helper.execute(model, device)\n",
    "\n",
    "    return train_loss / len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch, mb):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        pb = progress_bar(test_loader, parent=mb)\n",
    "        for data, _ in test_loader:\n",
    "            data = data.to(device)\n",
    "            data = data.view(-1, 784)\n",
    "            recon_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "\n",
    "    return test_loss / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, epochs, figure_interval, viz_helper, device):\n",
    "    mb = master_bar(range(1, epochs + 1))\n",
    "    viz_helper.execute(model, device)\n",
    "    for epoch in mb:\n",
    "        trn_loss = train(epoch, mb, figure_interval=10, viz_helper=viz_helper, device=device)\n",
    "        tst_loss = test(epoch, mb)\n",
    "        mb.write(f'epoch {epoch}, train loss: {round(trn_loss,6)}, test loss: {round(tst_loss, 6)}')                                                                  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE with 20-d Latent Space\n",
    "\n",
    "Use this one to generate reconstruction figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_helper_20d = VAEVizHelper(recon_base_imgs, datagen_tracking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BernoulliVAE(data_size=784, encoder_szs=[400], latent_size=20, \n",
    "                     decoder_szs=[400]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliVAE(\n",
       "  (encoder): VAEEncoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (encoder_mu): Linear(in_features=400, out_features=20, bias=True)\n",
       "    (encoder_logvar): Linear(in_features=400, out_features=20, bias=True)\n",
       "  )\n",
       "  (decoder): BernoulliVAEDecoder(\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=20, out_features=400, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=400, out_features=784, bias=True)\n",
       "      (3): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "epoch 1, train loss: 220.116325, test loss: 160.278555<p>epoch 2, train loss: 144.191065, test loss: 129.848067<p>epoch 3, train loss: 123.336315, test loss: 117.012127<p>epoch 4, train loss: 113.363996, test loss: 108.841106<p>epoch 5, train loss: 107.265231, test loss: 104.252984<p>epoch 6, train loss: 103.150955, test loss: 100.830625<p>epoch 7, train loss: 100.358333, test loss: 98.358381<p>epoch 8, train loss: 98.212018, test loss: 96.576907<p>epoch 9, train loss: 96.471595, test loss: 95.263134<p>epoch 10, train loss: 95.126471, test loss: 94.029714"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit(model, 10, 10, viz_helper_20d, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('~/git/acetherace.github.io/images/vae/recon_tracking.gif', \n",
    "                viz_helper_20d.recon_tracking_imgs[::5], duration=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('~/git/acetherace.github.io/images/vae/recon_tracking_early.gif',\n",
    "                viz_helper_20d.recon_tracking_imgs[:150:1], duration=.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anomaly = imageio.imread('~/git/acetherace.github.io/images/vae/mario.png', as_gray=True)\n",
    "# anomaly = cv2.resize(anomaly, dsize=(28,28))\n",
    "# anomaly /= 255\n",
    "# anomaly = (anomaly > 0.5).astype('int')\n",
    "# anomaly = 1 - anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly = np.random.randint(low=0, high=2, size=(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd499352400>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAANmUlEQVR4nO3dT4gk533G8eeJbF9kH1bRarPI6/gPOkQYss4MS0AhyIQYWZeVDw7WwWyIyfhggQ05RCgHC0JAhNjBh2AYW8Jr48gYJEV7MLHFYiJ8MeoRG2mVdSJFbOz1DvsHHSyfHEm/HKYURqvurla/9fZbvb/vB4aeqemq+k1tP1vV/db7vo4IAbj+/VbrAgCsBmEHkiDsQBKEHUiCsANJvGuVO7Nd7aP/jY2NovV3dnaW3n7fuqX6/rZ5+y9Zdwil/y7zlNTe+riUmFf7+fPndfXqVU/7nUua3mzfJelrkm6Q9M2IeKjn+dXCXtqEaE89Pgttv2/dUn1/27z9l6w7hJpNuyW1tz4uJebVvrm5qclkMrX4pS/jbd8g6Z8kfVLS7ZLutX37stsDUFfJe/Zjkl6KiJcj4jeSvifp+DBlARhaSdhvlfSLfT9f6Ja9he0t2xPbk4J9AShU8gHdtPcFb3szERHbkraluu/ZAcxXcma/IOnIvp/fL+liWTkAaikJ+zOSbrP9IdvvkfQZSaeGKQvA0Ja+jI+I12zfJ+mH2mt6eyQiXigppmZzSEnTWum+S7X8u9dZzebSmq+XWtsuamd/xzvrec/esj15zGGvaZ3D3vLeiDGHPSKGbWcHsF4IO5AEYQeSIOxAEoQdSIKwA0mstD97n5rtyaXrF3YFrrbtUuvcJDnm2vqUNAv2dXGdhTM7kARhB5Ig7EAShB1IgrADSRB2IImVNr1tbGxoMpk9OlXNXkxZu8+uc6+2PmNuWitR6/XCmR1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkrhuRpctVXNk2z5j7gK7zlrObtvq9VRlFlcA64WwA0kQdiAJwg4kQdiBJAg7kARhB5IY1VDSJVq20Y+5X3XrNvya9yf0qTn+QakWtRWF3fZ5Sa9Kel3SaxExe9BqAE0NcWb/eERcHWA7ACriPTuQRGnYQ9KPbO/Y3pr2BNtbtie2Zw8+B6C60sv4OyLiou1bJD1l+2cR8fT+J0TEtqRtqb8jDIB6is7sEXGxe7ws6QlJx4YoCsDwlg677Rttv+/N7yV9QtLZoQoDMKySy/hDkp7o2gTfJemfI+JfS4pp3fa57L5b192yr33Nbde8d2Kdp3te1tJhj4iXJf3+gLUAqIimNyAJwg4kQdiBJAg7kARhB5JYqy6uJU1MNOO0UdJkWbLtvu23bi4twZTNAOYi7EAShB1IgrADSRB2IAnCDiRB2IEkRtXOXtK2Wbstu2a7a81uqK3bk9d1KOk+NY9rybY3N2cP8MyZHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSGFU7e0n7Yu0hk2v2pe8z5uGcW2r5eikdw6DFceXMDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJrLSdfWNjQ5PJZOn1a47tXrJ+7fHPS9p0x9zGP+bx9mu3k7f423rP7LYfsX3Z9tl9y26y/ZTtF7vHA3XLBFBqkcv4b0m665pl90s6HRG3STrd/QxgxHrDHhFPS3rlmsXHJZ3svj8p6Z6B6wIwsGU/oDsUEbuS1D3eMuuJtrdsT2xPrly5suTuAJSq/ml8RGxHxGZEbB48eLD27gDMsGzYL9k+LEnd4+XhSgJQw7JhPyXpRPf9CUlPDlMOgFq8QFvno5LulHSzpEuSvizpXyR9X9IHJP1c0qcj4toP8aZta+7OWrYn92m57xKt24tbjZ/et/6Y+6MPcN/G1A30hn1IhH31CPvw6y6yfolaYed2WSAJwg4kQdiBJAg7kARhB5IYVRfXkk9Aa3czLdGytjG3FLTUsvtsK5zZgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiCJUU3ZXLNNeMxDSfcZ4/S/Q6jZq61U7X3Xundic3Nz5u84swNJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEittZ9/Z2ak64metdaVx3wNQouXos2Me4bXPOt7bwJkdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5IY1bjxfcbcHj3PmGeYrdnPv1TNdviMbfi9Z3bbj9i+bPvsvmUP2v6l7TPd191VqgMwmEUu478l6a4py/8xIo52Xz8YtiwAQ+sNe0Q8LemVFdQCoKKSD+jus/1cd5l/YNaTbG/ZntieXLlypWB3AEosG/avS/qIpKOSdiV9ZdYTI2I7IjYjYvPgwYNL7g5AqaXCHhGXIuL1iHhD0jckHRu2LABDWyrstg/v+/FTks7Oei6AcehtZ7f9qKQ7Jd1s+4KkL0u60/ZRSSHpvKTPD1HMmNtNS9Zf57nAW7ZHlx63dT3uteruDXtE3Dtl8cMVagFQEbfLAkkQdiAJwg4kQdiBJAg7kMSopmyuaaxT7C6iZvPXmJvWWg41PebXy7K1cWYHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSRGNWUzplvH6YEXsc5DbPe9jmu+zufVtrm5OfN3nNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IIlRTdlc0nZZqw/wEPu+nq1rX/vSbbecTpr+7ADmIuxAEoQdSIKwA0kQdiAJwg4kQdiBJEbVn32sbbJ922/dR39d2/nHPG58qZr3fVTrz277iO0f2z5n+wXbX+yW32T7Kdsvdo8HlikcwGoschn/mqS/iojfk/SHkr5g+3ZJ90s6HRG3STrd/QxgpHrDHhG7EfFs9/2rks5JulXScUknu6edlHRPrSIBlHtH79ltf1DSxyT9VNKhiNiV9v5DsH3LjHW2JG2VlQmg1MJht/1eSY9J+lJE/GrRDxgiYlvSdreN9fwkCbgOLNT0Zvvd2gv6dyPi8W7xJduHu98flnS5TokAhtB7ZvfeKfxhSeci4qv7fnVK0glJD3WPT5YWU6s5onTbi2y/ZN81mwVbTxfdakjlUtdjs+Ail/F3SPqspOdtn+mWPaC9kH/f9uck/VzSp+uUCGAIvWGPiJ9ImvXf1J8MWw6AWrhdFkiCsANJEHYgCcIOJEHYgSRW2sW1T0nbZe1upjW333Lb69wNdMxde0v+tlr3LnBmB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEk1mrK5hIt23RbDjU95rboPjVrL3091JzyuWTbRUNJA7g+EHYgCcIOJEHYgSQIO5AEYQeSIOxAEl5lO2zpjDAt27rH3F5dc9z4kn2vYv/zXK/HZYFtT30CZ3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSGKR+dmPSPq2pN+R9Iak7Yj4mu0HJf2lpCvdUx+IiB+UFFNzrO3Sfc9Tu796q77RtbU+bvPUPm4t7gHovanG9mFJhyPiWdvvk7Qj6R5Jfybp1xHxDwvvrOemmpYD669z2Odp+aLt2/71fNxK/rYBXstTn7DI/Oy7kna771+1fU7SrX3rARiXd/Se3fYHJX1M0k+7RffZfs72I7YPzFhny/bE9uzxqABUt/C98bbfK+nfJP1dRDxu+5Ckq5JC0t9q71L/L3q2wWX8EriMXw6X8W+10Jnd9rslPSbpuxHxeLfBSxHxekS8Iekbko4tsi0AbfSG3Xv/jTws6VxEfHXf8sP7nvYpSWeHLw/AUBYZSvoOSZ+V9LztM92yByTda/uo9i7jz0v6fGkxJZc+tS+75q0/5qmHx9w1t6XStxBj/TedN5T0Ip/G/0TStL+sqE0dwGpxBx2QBGEHkiDsQBKEHUiCsANJEHYgiVFN2dxSy+61fcY8lXXLfZcc99r7HmPXYs7sQBKEHUiCsANJEHYgCcIOJEHYgSQIO5DEqqdsviLpf/Ytull7Q1uN0VhrG2tdErUta8jafjciDk77xUrD/rad25OImN3bvqGx1jbWuiRqW9aqauMyHkiCsANJtA77duP9zzPW2sZal0Rty1pJbU3fswNYndZndgArQtiBJJqE3fZdtv/T9ku2729Rwyy2z9t+3vaZ1vPTdXPoXbZ9dt+ym2w/ZfvF7nHqHHuNanvQ9i+7Y3fG9t2Najti+8e2z9l+wfYXu+VNj92culZy3Fb+nt32DZL+S9KfSrog6RlJ90bEf6y0kBlsn5e0GRHNb8Cw/ceSfi3p2xHx0W7Z30t6JSIe6v6jPBARfz2S2h7UO5zGu1Jts6YZ/3M1PHZDTn++jBZn9mOSXoqIlyPiN5K+J+l4gzpGLyKelvTKNYuPSzrZfX9Sey+WlZtR2yhExG5EPNt9/6qkN6cZb3rs5tS1Ei3CfqukX+z7+YLGNd97SPqR7R3bW62LmeJQROxKey8eSbc0rudavdN4r9I104yP5tgtM/15qRZhnzY415ja/+6IiD+Q9ElJX+guV7GYr0v6iKSjknYlfaVlMd00449J+lJE/KplLftNqWslx61F2C9IOrLv5/dLutigjqki4mL3eFnSExrfVNSX3pxBt3u83Lie/zemabynTTOuERy7ltOftwj7M5Jus/0h2++R9BlJpxrU8Ta2b+w+OJHtGyV9QuObivqUpBPd9yckPdmwlrcYyzTes6YZV+Nj13z684hY+Zeku7X3ifx/S/qbFjXMqOvDkv69+3qhdW2SHtXeZd3/au+K6HOSflvSaUkvdo83jai270h6XtJz2gvW4Ua1/ZH23ho+J+lM93V362M3p66VHDdulwWS4A46IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUji/wCXMzRpXLu+oQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(anomaly, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence(mu, logvar):\n",
    "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "kls = []\n",
    "for i in range(len(train_loader.dataset)):\n",
    "    x, _ = train_loader.dataset[i]\n",
    "    x = x.view(-1,784)\n",
    "    _, mu, logvar = model(x.to(device))\n",
    "    kls.append(kl_divergence(mu, logvar).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kls = np.asarray(kls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.75917435, 27.2908535 , 26.58039093, 19.9089241 , 28.09859467])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.distplot(kls, kde=False, ax=ax)\n",
    "ax.set_title('Distribution of KL Divergences in Training Set')\n",
    "ax.set_xlabel('KL Divergence')\n",
    "ax.set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78.3709716796875"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, mu, logvar = model(torch.tensor(anomaly).type(torch.FloatTensor).view(-1,784).to(device))\n",
    "kl_divergence(mu, logvar).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, std = norm.fit(kls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27.574872059281667, 4.384177863044052)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE with 2-d Latent Space\n",
    "\n",
    "Use this one to generate data generation figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_helper_2d = VAEVizHelper(recon_base_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BernoulliVAE(data_size=784, encoder_szs=[400,150], latent_size=2,\n",
    "                     decoder_szs=[150,400]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliVAE(\n",
       "  (encoder): VAEEncoder(\n",
       "    (encoder): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=400, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=400, out_features=150, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (encoder_mu): Linear(in_features=150, out_features=2, bias=True)\n",
       "    (encoder_logvar): Linear(in_features=150, out_features=2, bias=True)\n",
       "  )\n",
       "  (decoder): BernoulliVAEDecoder(\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=2, out_features=150, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=150, out_features=400, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=400, out_features=784, bias=True)\n",
       "      (5): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "epoch 1, train loss: 222.910095, test loss: 179.727125<p>epoch 2, train loss: 172.362153, test loss: 167.05883<p>epoch 3, train loss: 164.388866, test loss: 162.923456<p>epoch 4, train loss: 160.547806, test loss: 159.208533<p>epoch 5, train loss: 157.457237, test loss: 156.529575<p>epoch 6, train loss: 154.8764, test loss: 154.684749<p>epoch 7, train loss: 152.67003, test loss: 154.017791<p>epoch 8, train loss: 150.910928, test loss: 151.228423<p>epoch 9, train loss: 149.290028, test loss: 150.167391<p>epoch 10, train loss: 147.889375, test loss: 148.963001"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit(model, 10, 10, viz_helper_2d, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('~/git/acetherace.github.io/images/vae/datagen_tracking.gif', \n",
    "                viz_helper_2d.datagen_tracking_imgs[::5], duration=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.mimsave('~/git/acetherace.github.io/images/vae/datagen_tracking_early.gif',\n",
    "                viz_helper_2d.datagen_tracking_imgs[:200:2], duration=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(viz_helper_2d.datagen_tracking_imgs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageio.imwrite('~/git/acetherace.github.io/images/vae/datagen_final.png', \n",
    "                viz_helper_2d.datagen_tracking_imgs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
